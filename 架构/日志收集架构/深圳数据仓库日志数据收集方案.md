# 总体方案

深圳数据仓库跟北京大数据组沟后结论：

1、深圳的业务日志的收集使用北京这边的Flume agent进行采集并写入北京的kafka集群，深圳这边提供日志的存储地址，由北京的同事部署和监控flume agent。

2、深圳同事负责业务日志数据kafka的清洗开发工作，统一按照北京的sparkstreaming开发规范清洗，并将清洗后的数据写入北京的OLAP数据库druid，北京同事提供sparkstreaming读Kafka和写druid的deamo的支持，深圳同事开发的sparkstreaming程序由北京的同事统一部署。

3、sparkstreaming程序协作开发方式，深圳这边提供一台机器，北京同事帮忙部署spark ,并以client端的方式连接到北京的spark集群，仅作为sparkstreaming程序的任务提交端使用，不参与集群的计算。

4、离线报表深圳的同事采用定时器定时调度sql脚本，生成报表数据到业务库，北京的同事提供druid的底层数据仓库的支持。

5、实时查询功能，深圳的同事开发前端查询界面，北京的同事提供druid的底层的数据仓库支持。
