# hdfs 完全分布式搭建

机器规划
![ss](assets/markdown-img-paste-20181120211727563.png)

1. 时间同步,网络免密码登录(namenode到所有的datanode,此处为node1==>node1-4),修改hosts文件,防火墙关闭

```
设置主机名:
vim /etc/sysconfig/network
vim /etc/hosts

安装时间同步
yum install ntp -y
ntpdate -u cn.pool.ntp.org
ntpdate -u ntp1.aliyun.com

防火墙关闭
/etc/init.d/iptables stop
setenforce 0

免密码登录(node01=>node01,node01=>node01-04)
在node01上执行:
ssh-keygen -t rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub node01
ssh-copy-id -i ~/.ssh/id_rsa.pub node02
ssh-copy-id -i ~/.ssh/id_rsa.pub node03
ssh-copy-id -i ~/.ssh/id_rsa.pub node04
```

2. 安装jdk
3. 上传安装文件,解压
4. 修改配置文件
    hadoop-env.sh 中的JAVA_HOME:
    ```
    export JAVA_HOME=/opt/jdk1.8.0_191

    ```
    core-site.xml
    ```
    <configuration>
    	<property>
    		<name>hadoop.tmp.dir</name>
    		<value>/opt/hadoop</value>
    	</property>
    	<property>
    		<name>fs.default.name</name>
    		<value>hdfs://node01:9000</value>
    	</property>
   </configuration>

    ```
    hdfs-site.xml
    ```
    <configuration>
    	<property>
    		<name>dfs.replication</name>
    		<value>3</value>
    	</property>
      <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node02:50090</value>
      </property>
  </configuration>

    ```
    slaves 指定datanode
    ```
    node02
    node03
    node04
    ```
    手动创建masters 指家SNN
    ```
    node02
    ```
5. 同步配置文件,保证集群每台服务器配置都相同
6. 格式化namenode hdfs namenode -format
7. 启动 start-dfs.sh


错误:
```
2018-11-21 05:36:15,187 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.net.UnknownHostException: bogon: bogon: Name or service not known
	at java.net.InetAddress.getLocalHost(InetAddress.java:1506)
	at org.apache.hadoop.security.SecurityUtil.getLocalHostName(SecurityUtil.java:187)
	at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:207)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2217)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2266)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2442)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2466)
Caused by: java.net.UnknownHostException: bogon: Name or service not known
	at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)
	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)
	at java.net.InetAddress.getLocalHost(InetAddress.java:1501)
	... 6 more

这是由于hostname没有设置成功的原因,要重新设置hostname
```

执行命令时报错:
```
18/11/22 06:50:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

```

原因可能是hadoop的预编译包与操作系统的不一致,比如hadoop的预编译包是32bit的而linux是64位.
我可以检查一下:
```
进入hadoop的lib/native目录
ldd libhadoop.so.1.0.0

./libhadoop.so.1.0.0: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by ./libhadoop.so.1.0.0)
	linux-vdso.so.1 =>  (0x00007fff705e3000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00007f41dba8c000)
	libc.so.6 => /lib64/libc.so.6 (0x00007f41db6f7000)
	/lib64/ld-linux-x86-64.so.2 (0x0000003724200000)

可见原来系统预装的glibc库是2.12版本，而hadoop期望是2.14版本，所以打印警告信息.

```

解决方法:安装glibc2.14

下载地址:http://ftp.gnu.org/gnu/glibc/
具体操作可看:https://www.cnblogs.com/erygreat/p/7223829.html;


# ha namenode

机器规划

![](assets/markdown-img-paste-20181123224556234.png)

1. 时间同步,jdk,修改hosts文件,防火墙关闭

2. 网络免密码登录

node01到 node01-04
node02 到 node01-04
两台NN 之间也要做免密钥登录
```
在node02上运行:
ssh-keygen -t rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub node01
ssh-copy-id -i ~/.ssh/id_rsa.pub node03
ssh-copy-id -i ~/.ssh/id_rsa.pub node04

```

3. 安装zookeeper

```
分别在node01,node02,node03三个节点上安装zookeeper
然后分别在三台机器上启动zk

```

4. 修改配置文件

```
1.x当中创建的masters文件一定要保证每台服务器都删除.

删除hadoop以前存放数据的文件,现在是:/opt/hadoop


```

修改hdfs-site.xml
```
<configuration>
	<property>
  		<name>dfs.replication</name>
    		<value>3</value>
    	</property>
    	<property>
    		<name>dfs.nameservices</name>
    	       <value>mycluster</value>
    	</property>
	<property>
	  	<name>dfs.ha.namenodes.mycluster</name>
	    	<value>nn1,nn2</value>
	</property>
	<property>
	  	<name>dfs.namenode.rpc-address.mycluster.nn1</name>
	    	<value>node01:8020</value>
	</property>
	<property>
	      	<name>dfs.namenode.rpc-address.mycluster.nn2</name>
	        <value>node02:8020</value>
	</property>
	<property>
	  	<name>dfs.namenode.http-address.mycluster.nn1</name>
	    	<value>node01:50070</value>
	</property>
	<property>
	      	<name>dfs.namenode.http-address.mycluster.nn2</name>
	        <value>node02:50070</value>
	</property>
	<property>
	  	<name>dfs.namenode.shared.edits.dir</name>
	    	<value>qjournal://node02:8485;node03:8485;node04:8485/mycluster</value>
	</property>
	<property>
	  	<name>dfs.client.failover.proxy.provider.mycluster</name>
	    	<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>
	<property>
	  	<name>dfs.ha.fencing.methods</name>
	    	<value>sshfence</value>
	</property>
	<property>
  		<name>dfs.ha.fencing.ssh.private-key-files</name>
    		<value>/home/root/.ssh/id_rsa</value>
    	</property>
    	<property>
    	  	<name>dfs.journalnode.edits.dir</name>
    	    	<value>/path/to/journal/node/local/data</value>
    	</property>
</configuration>


```



core-site.xml:
```
<configuration>
	<property>
		<name>fs.default.name</name>
		<value>hdfs://mycluster</value>
	</property>
	<property>
	        <name>hadoop.tmp.dir</name>
	        <value>/opt/hadoop</value>
	</property>

</configuration>

```

5. 同步所有的配置文件到node01-04

6. 启动

分别在node02,node03,node04启动JournalNodes
```
hadoop-daemon.sh  start journalnode

```

格式化namenode,先在一台机器上执行,比如在node01
```
hdfs namenode -format

```

将node01上的namenode的元数据拷贝到另一台namenode上(node02)
```
hdfs namenode -bootstrapStandby

发现报错
18/11/23 20:05:36 FATAL ha.BootstrapStandby: Unable to fetch namespace information from active NN at node01/192.168.152.3:8020: Call From node02/192.168.152.4 to node01:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

原因是因为node01上的namenode服务没有启动,所以一直连接失败,只要将node01上的namenode服务启动即可.

在node01在node01上启动namenode

hadoop-daemon.sh  start namenode
```

格式化zookeeper

```
hdfs zkfc -formatZK

```

启动hdfs
```
start-dfs.sh

```

# yarn 部署及高可用

![](assets/markdown-img-paste-20181124173926502.png)

1. mapred-site.xml

```

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

```

2. yarn-site.xml

```
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
  <property>
   <name>yarn.resourcemanager.ha.enabled</name>
   <value>true</value>
 </property>
 <property>
   <name>yarn.resourcemanager.cluster-id</name>
   <value>cluster1</value>
 </property>
 <property>
   <name>yarn.resourcemanager.ha.rm-ids</name>
   <value>rm1,rm2</value>
 </property>
 <property>
   <name>yarn.resourcemanager.hostname.rm1</name>
   <value>node03</value>
 </property>
 <property>
   <name>yarn.resourcemanager.hostname.rm2</name>
   <value>node04</value>
 </property>
 <property>
   <name>yarn.resourcemanager.zk-address</name>
   <value>node01:2181,node02:2181,node03:2181</value>
 </property>
</configuration>

```

3. 同步配置文件到node01-04

4. 启动

```
start-dfs.sh
start-yarn.sh

或者 start-all.sh

最后发现在resourcemanager并没有启动,因为这个要手动启动才行,脚本启动不了,这是sh脚本的一个bug.

在node03,node04分别执行:

yarn-daemon.sh start resourcemanager

```

# 安装hive

1. 安装mysql

安装
```
yum install mysql-server

启动
service mysqld start

设置开机启动
chkconfig mysqld on

```
权限用户配置

```

GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123' WITH GRANT OPTION;

flush privileges;

删除其它多余的用户认证

delete from user where password ='';
```

解压hive安装包apache-hive-1.2.2-bin.tar.gz

修改 配置文件
hive-site.xml
```
<?xml version="1.0"?>  
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>  

<configuration>  
<property>  
  <name>hive.metastore.warehouse.dir</name>  
  <value>/user/hive_remote/warehouse</value>  
</property>  
<property>  
  <name>hive.metastore.local</name>  
  <value>true</value>  
</property>  
<property>  
  <name>javax.jdo.option.ConnectionURL</name>  
  <value>jdbc:mysql://node01/hive_remote?createDatabaseIfNotExist=true</value>  
</property>  
<property>  
  <name>javax.jdo.option.ConnectionDriverName</name>  
  <value>com.mysql.jdbc.Driver</value>  
</property>  
<property>  
  <name>javax.jdo.option.ConnectionUserName</name>  
  <value>root</value>  
</property>  
<property>  
  <name>javax.jdo.option.ConnectionPassword</name>  
  <value>123</value>  
</property>  
</configuration>  


```

将mysql的驱动包放入hive的lib目录下.

启动
```
hive

```

报错
```
[ERROR] Terminal initialization failed; falling back to unsupported
java.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected
	at jline.TerminalFactory.create(TerminalFactory.java:101)
	at jline.TerminalFactory.get(TerminalFactory.java:158)
	at jline.console.ConsoleReader.<init>(ConsoleReader.java:229)
	at jline.console.ConsoleReader.<init>(ConsoleReader.java:221)
	at jline.console.ConsoleReader.<init>(ConsoleReader.java:209)
	at org.apache.hadoop.hive.cli.CliDriver.setupConsoleReader(CliDriver.java:787)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:721)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

原因是hive中的jline包与我们hadoop环境/home/hadoop-2.6.5/share/hadoop/yarn/将mysql的驱动包放入hive的lib目录下的jar包冲突了,比如
jline-0.9.94.jar,jline-2.12.jar

解决方法:将hadoop中的底版本删除,并将hive中的高版本的jline-2.12.jar 复制过来.
```

设置hadoop的环境变量,这里已经设置了HADOOP_HOME环境变理.hive可以通过此环境变量关联到hadoop

再次启动 hive,启动成功.

可以用以下语句测试是否安装成功
```
create table tbl0 (id int,age int);
insert into tbl0 values(1,9);
select * from tbl0;

```

# hive 多用户模式搭建

机器分配:node01--mysql ,node03--MetaStoreService,node04---hiveclient

将node01上的hive安装包传到node03和node04上.

node03修改配置文件:
```

<configuration>  

<property>  
  <name>hive.metastore.warehouse.dir</name>  
  <value>/user/hive/warehouse</value>  
</property>  
<property>  
  <name>javax.jdo.option.ConnectionURL</name>  
  <value>jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true</value>  
</property>  
<property>  
  <name>javax.jdo.option.ConnectionDriverName</name>  
  <value>com.mysql.jdbc.Driver</value>  
</property>  
<property>  
  <name>javax.jdo.option.ConnectionUserName</name>  
  <value>root</value>  
</property>  
<property>  
  <name>javax.jdo.option.ConnectionPassword</name>  
  <value>123</value>  
</property>  
</configuration>  


```

启动metaStoreService
```

hive --service metastore   

启动成功之会,监听9083这个端口.

```

配置node04---hiveclient
```
<configuration>  
<property>  
  <name>hive.metastore.warehouse.dir</name>  
  <value>/user/hive/warehouse</value>  
</property>  
<property>  
  <name>hive.metastore.local</name>  
  <value>false</value>  
</property>  
<property>  
  <name>hive.metastore.uris</name>  
  <value>thrift://node03:9083</value>  
</property>  
</configuration>  


```

最后启动hive client
```

hive

测试
create table tbl0 (id int,age int);
insert into tbl0 values(1,9);
select * from tbl0;


```


# 安装spark(在yarn上运行)

1. 下载安装包

2. 解压安装包spark-2.2.0-bin-hadoop2.6.tgz

3. 修改配置文件

spark-env.sh
```
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_HOME=/home/spark-2.2.0-bin-hadoop2.6
export JAVA_HOME=/opt/jdk1.8.0_191
export HADOOP_HOME=/home/hadoop-2.6.5

```

# spark+hive

```

将hive 的hive-site.xml拷贝到spark/conf目录下.

```

最后还有一些告警,没有找到办法,在此乎略不计.

测试代码
```

import org.apache.spark.sql.hive.HiveContext
val hiveContext = new HiveContext(sc)
hiveContext.sql("select * from tbl0").show()

```

# spark 开发
